{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#amazon-eks-anywhere-eks-a-conformance-and-validation-framework","title":"Amazon EKS Anywhere (EKS-A) Conformance and Validation Framework","text":"<p>\ud83d\udca5 Welcome to Amazon EKS Anywhere (EKS-A) Conformance and Validation Framework \ud83d\udca5</p> <p>\ud83c\udfaf This repository is part of the Amazon EKS Anywhere (EKS-A) Conformance and Validation Framework, designed to address general validation and quality assurance of Partner and third-party solutions (add-ons) running on EKS-A on supported operating systems, hardware and virtualization platforms.</p> <p>\ud83c\udfaf The EKS Anywhere conformance and validation framework provides an expandable and extensible approach to run conformance testing on different EKS deployment models such as EKS-A on VMware (VMC), EKS-A on Bare Metal, EKS-A on Snow , EKS-A on Nutanix and Local clusters for Amazon EKS on AWS Outposts. It allows running Kubernetes conformance testing, Partner and OSS add-on deployment and validation on EKS-A environments and helps Partners validate their hardware (IHV) and software (ISV) solutions deployed on variety of EKS environments.</p> <p>\ud83c\udfaf This repository is a GitOps repository powered by FluxCD and contains Partner and third-party solutions and functional tests for deployment in the supported deployment environments. Each deployment option is represented by the respective folder in this repository, where Partners and external contributors can submit a pull request.</p> <p>\ud83c\udfaf GitOps is leveraged as a decoupling mechanism between physical test environments and ISV solutions, enabling Partners to test their solutions without direct access to the respective labs and avoid potentially costly maintenance of the test environments.</p>"},{"location":"#getting-started","title":"\ud83c\udfc3\u200d\u2640\ufe0fGetting Started","text":"<p>Deployment of a third-party solution requires a PR for a FluxCD deployment submitted to this repository.</p> <p>\ud83d\ude80 The framework allows to submit your solution to a single location and deploy across all environments with the same configuration. In this case, create a new solution specific folder (e.g. <code>&lt;orgname&gt;</code> or <code>&lt;orgname&gt;/&lt;productname&gt;</code>) in the common folder eks-anywhere-common/Addons/Partner and submit your GitOps deployment (e.g. HelmRelease, manifests and/or other support package management resources) in that folder.</p> <p>\ud83d\ude80 If your product and/or configuration must be distinct for each of the deployment options then create a new solution under the respective target. For example, if it is for EKS-A on Snow then the path is <code>eks-anywhere-snow/Addons/Partner</code>.</p> <p>\ud83d\ude80 For kubernetes namespace resource for your product, please add labels as shown below for reporting purposes:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n    name: kubecost\n    labels:\n        aws.conformance.vendor: kubecost\n        aws.conformance.vendor-solution: cost-analyzer\n        aws.conformance.vendor-solution-version: 2.0.2\n</code></pre> <p>\ud83d\ude80 You can deploy Helm via FluxCD HelmRelease custom resource. Here is a Helm example. In particular the example covers specification of Helm repository and Helm release.</p> <p>\ud83d\ude80 Secrets management such as license key or credentials is implemented using the External Secrets add-on. You will need to share secrets with the AWS Partner team. The AWS Partner team will create those secrets in an AWS account and use External Secrets to bring them down to the target deployment cluster. After that, such secrets can be configured in your GitOps deployment folder and passed to the deployment using configuration values or if your helm deployment can use pre-created secrets, that option is also supported.  The sample folder also contains an example of leveraging a secret with the deployment as well as an example of wiring that secret in your deployment here (line numbers may change in the link).</p> <p>\ud83d\ude80 Though deployment and validation of your solution on the target deployment option is helpful, it does not provide the required level of quality assurance for functional verification, which is generally achieved with a test framework and automation normally included in the CI/CD cycle of the Partner product.</p>"},{"location":"#functional-job-requirements","title":"Functional Job Requirements","text":"<ol> <li>Functional test should base its test cases on the specifications of the ISV product under test</li> <li>Functional test should validate the functionality of the ISV product and describe what the ISV product does</li> <li>Healthchecks, service endpoints checks or any other technical checks do not represent sufficient coverage required for the functional test</li> <li>Functional test should be wrapped as a container, container image should be published on ECR, and/or provide evidence of successful recent vulnerability scan</li> <li>Functional test must be implemented as a Kubernetes CronJob and any non-zero exit status of the job execution will be considered a failure</li> <li>Functional test must be repeatable. That means that if the job has executed before successfully and no changes were applied, we expect to run it continuously and mark the product as failure if the test job starts producing failures even if previous executions against the same environment were successful. Functional test CronJobs should be configured to run at least once daily</li> <li>Functional test should not require elevated security permissions, such as cluster roles, privileged mode, non-ephemeral storage</li> <li>Functional test should be submitted under <code>eks-anywhere-common/testers</code> (runs on all platforms) or under your respective environment folder such as <code>eks-anywhere-snow/testers</code> (e.g. <code>eks-anywhere-snow/testers/&lt;orgname&gt;/&lt;productname&gt;</code>)</li> </ol> <p>Refer the example here for functional test cronjob.</p>"},{"location":"#contribution-flow","title":"Contribution Flow","text":"<ol> <li>Fork the repo.</li> <li>Apply changes such as deployment and/or any documentation.</li> <li>Test them locally using FluxCD.</li> <li>Submit a PR to the main branch of this repository.</li> </ol>"},{"location":"#pre-requisite-linuxmacos","title":"Pre-requisite (Linux/MacOS)","text":"<p>Please use the dev container configuration in the <code>.devcontainer</code> folder with devpod or any other dev container environment to create minikube cluster with all the required pre-requisites such as helm, kubectl and flux for local testing. You can skip over to the local testing section if you use dev container environment.</p> <p>This solution requires Flux CLI locally and Flux Controller on your Kubernetes cluster. Flux requires access to a source repository via api and access to the kubernetes cluster you want to use for testing. Please follow the below steps for installing these pre-requisites.</p> <p>If you do not already have access to a running kubernetes cluster you can consider setting up an EKS Anywhere local cluster on docker provider or a local k3s cluster or you may choose a hosted service such as AWS EKS.</p> <p>Flux integrates into your running cluster and needs the kubeconfig file of the cluster for testing. Flux will look in the default location, i.e. ~/.kube/config.</p> <p>Before setting up Flux make sure your configuration file points to your cluster. You can use the following command for example to verify that a suitable kubeconfig file can be found and the cluster can be accessed. If no configuration is found you will get an error message indicating that \"http://localhost:8080/version\" cannot be accessed. Do not be confused by the port number. The port number for accessing the kubernetes cluster is part of the configuration file and the reported port in the error message is a default port.</p> <pre><code>kubectl get ns\n</code></pre> <p>You can use the following to ensure the flux installation finds the cluster you want to use for testing.</p> <pre><code>export KUBECONFIG=$PATH_TO_kubeconfig.yaml\n</code></pre> <p>Once you have a kubernetes cluster running and the configuration file is properly setup you are ready to install flux.</p> <pre><code>git clone https://github.com/aws-samples/eks-anywhere-addons.git\ncd eks-anywhere-addons\nchmod +x installFlux.sh\n./installFlux.sh\n</code></pre> <p>Note: In order to commit back to the project you need to create a fork in GitHub of the eks-anywhere-addons repository into your own project. After the fork is created clone the source code from your fork. To keep the projects connected, you can add the AWS project as an upstream by adding he below to your .git/config file.</p> <pre><code>[remote \"upstream\"]\n    url = git@github.com:aws-samples/eks-anywhere-addons.git\n    fetch = +refs/heads/*:refs/remotes/upstream/*\n</code></pre>"},{"location":"#local-testing-linuxmacos","title":"Local Testing (Linux/MacOS)","text":"<p>\ud83d\ude80 First, lets create a plaintext secret in AWS Secrets Manager for your secrets (credentials, license keys, API keys, etc.) using instructions in create an AWS Secrets Manager secret in <code>us-west-2</code> region.</p> <p>Next, install external-secrets on your cluster using the below helm installation of external-secrets to sync secrets between AWS Secrets manager and EKSA Cluster:</p> <pre><code>helm repo add external-secrets https://charts.external-secrets.io\n\nhelm install external-secrets \\\n    external-secrets/external-secrets \\\n    -n external-secrets \\\n    --create-namespace \n</code></pre> <p>Next, lets create following Kubernetes generic secret in to your cluster for setting ClusterSecretStore to access AWS Secrets Manager to pull your secrets required for installation of your product:</p> <pre><code>aws iam create-access-key \\\n  --user-name external-secrets-${EKS_DEPLOYMENT_MODEL_SHORT} &gt; aws_creds.json\n\nACCESS_KEY=$(cat aws_creds.json | jq -r .AccessKey.AccessKeyId)\nSECRET_KEY=$(cat aws_creds.json | jq -r .AccessKey.SecretAccessKey)\n\nkubectl create secret generic aws-secret \\\n  --from-literal=access-key=$ACCESS_KEY \\\n  --from-literal=secret=$SECRET_KEY\n\nrm -rf aws_creds.json\n</code></pre> <p>Next, lets create the following Kubernetes resource <code>ClusterSecretStore</code> to to access AWS Secrets Manager to pull your secrets required for installation of your product:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f - \napiVersion: external-secrets.io/v1\nkind: ClusterSecretStore\nmetadata:\n  name: eksa-configmap-store\nspec:\n  provider:\n    aws:  # set secretStore provider to AWS.\n      service: SecretsManager # Configure service to be Secrets Manager\n      region: us-west-2  # Region where the secret is.\n      auth:\n        secretRef:\n          accessKeyIDSecretRef: \n            name: aws-secret # References the secret we created\n            namespace: default\n            key: access-key  \n          secretAccessKeySecretRef:\n            name: aws-secret\n            namespace: default\n            key: secret\nEOF\n</code></pre> <p>Add <code>GitRepository</code> for your fork:</p> <pre><code>flux create source git addons \\\n    --url=&lt;forked repo from https://github.com/aws-samples/eks-anywhere-addons&gt;\\\n    --branch=main # This should be replaced with your branch for testing your changes\n</code></pre> <p>This creates a flux GitRepository resource. The flux GitRepository resource will periodically check the configured repo and branch for changes and sync any new commits. Since this project uses git, we are creating a <code>GitRepository</code> resource.</p> <p>Note: If you need/want to work in a disconnected fashion you need to run a git server on your system and push the eks-anywhere-addons repository to your git server.</p> <p>The name in the above command, addons, is arbitrary but must match the name used as the value of the --source argument in the following command.</p> <p>\ud83d\ude80 Add Kustomization for your add-on:</p> <pre><code># Example for Snowball Edge (replace --path with the target env as required)\nflux create kustomization addons-snow-partner \\\n    --source=addons \\\n    --path=\"./eks-anywhere-snow/Addons/Partner\" \\\n    --prune=true \\\n    --interval=5m \n</code></pre> <p>The given example will attempt to deploy all solutions it can find in the ./eks-anywhere-snow/Addons/Partner directory tree. You can limit your testing to only your application by providing a more specific path, ./eks-anywhere-common/Addons/Partner/foobar for example will deploy anything found in the foobar subdirectory. The --path setting must match the location where your deployment is setup, not the location for the testJob.</p> <p>The name, in this example addons-snow-partner, is arbitrary. As mentioned the value for the --source argument must match the name given when the source reference was created.</p>"},{"location":"#validation","title":"Validation","text":"<p>\ud83d\ude80 Validate by navigating to the target namespace and checking if all pods are running. As an example, Please see the kubernetes resources in botkube namespace as shown below:</p> <pre><code>\u276f kga -n botkube\nNAME                                   READY   STATUS    RESTARTS   AGE\npod/botkube-botkube-58c4579b44-87mbq   1/1     Running   0          7h55m\n\nNAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/botkube-botkube   1/1     1            1           7h55m\n\nNAME                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/botkube-botkube-58c4579b44   1         1         1       7h55m\n</code></pre> <p>For functional testing you need to create a testJob. See eks-anywhere-addons/eks-anywhere-common/Testers for examples. The descriptions about Jobs from upstream kubernetes provides helpful information and additional details.</p> <p>Presumably your application will provide a service that can be accessed to verify the application deployed as expected. In the test job performs a basic functional verification of your deployed product. The generic access pattern to access a service in the cluster is <code>servicename.namespace.svc.cluster.local:port</code> where <code>servicename</code> is the name of your service, <code>namespace</code> is the namespace you assigned in your deployment yaml description and <code>port</code> is the port number your service is running on.</p> <p>If you are uncertain about the service name you can use the following where $NAMESPACE is the namespace you assigned in your deployment yaml description.</p> <pre><code>kubectl get services -n $NAMESPACE\n</code></pre> <p>\ud83d\ude80 Add Kustomization for testing your test job :</p> <pre><code>flux create kustomization addons-snow-partner-testers \\\n    --source=addons \\\n    --path=\"./eks-anywhere-snow/Testers/Partner\" \\\n    --prune=true \\\n    --interval=5m \n</code></pre> <p>Use the below command to delete the existing job :</p> <pre><code>kubectl delete job $NAME_OF_TESTJOB -n $NAMESPACE\n</code></pre> <p>To debug the test job use the <code>kubectl logs</code> command.</p>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>TheFluxCD Troubleshooting guide provides information on most commands to troubleshoot the deployment and helps you understand any issues you may encounter.</p>"},{"location":"#support-feedback","title":"\ud83e\udd1d Support &amp; Feedback","text":"<p>Amazon EKS Anywhere (EKS-A) Conformance and Validation Framework is maintained by AWS Solution Architects and is not an AWS service. Support is provided on a best effort basis. If you have feedback, feature ideas, or wish to report bugs, please use the Issues section of this GitHub.</p>"},{"location":"#security","title":"Security","text":"<p>See CONTRIBUTING for more information.</p>"},{"location":"#license","title":"License","text":"<p>This library is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"#community","title":"\ud83d\ude4c Community","text":"<p>We welcome all individuals who are enthusiastic about Kubernetes to become a part of this open source conformance framework. Your contributions and participation are invaluable to the success of this project.</p>"},{"location":"#collaboration","title":"\ud83d\ude4c Collaboration","text":"<p>Please join us on slack at AWS Developers. Get onboarded to slack by sharing your emails with us.</p> <p>Built with \u2764\ufe0f at AWS.</p>"}]}